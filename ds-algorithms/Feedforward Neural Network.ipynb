{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### y = Fnn(x) = F3(F2(f1(x)))\n",
    "More generally\n",
    "##### Fl(z) = Gl(Wl*z+Bl)\n",
    "W -> matrix holding weights for a layer. This is a matrix because the length of a particular row needs to match the length of the vector z. It just works out this way, mathematically, even though it seems odd. So, a weight for a particular unit is duplicated across each row of Wl.\n",
    "\n",
    "z -> input vector data\n",
    "\n",
    "B -> bias vector for a particular layer\n",
    "\n",
    "G -> activation function defined for a particular layer. Turns linear transformation on a vector into a real number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Problems: vanishing and exploding gradients. Wth back propagation, chain rule is applied through layers of NN. Lots of matrix multiplications which causes small gradients to be smaller and large gradients to be huge.\n",
    "# Exploding gradients solved with gradient clipping or L1/L2 regularization. \n",
    "# Vanishing gradients a bigger problem for some time. Examples of solutions: ReLu, LSTM (or other gated units), skip connections, and modifications to gradient descent. \n",
    "\n",
    "# Activation functions:\n",
    "# Purpose: map the output of a unit to a domain we care about (e.g., (0,1) or (-1, 1))\n",
    "# Sigmoid: A differentiable function good for predicting probability, range (0, 1). Monotonic but not for derivative. Good for classification.\n",
    "# Tanh: A sigmoidal (and differentiable) function but range is from (-1, 1). Monotonic but not for derivative. Good for classification.\n",
    "# ReLu (Rectified Linear Unit): Most common (used in all Conv. NN). 0 when x < 0, x=x otherwise. Range (0, inf) Function and derivative are monotonic. \n",
    "# Some more information with derivatives of functions: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61134166 0.37097371 0.77215203 0.17046251]\n",
      " [0.38451581 0.34695895 0.4318071  0.78177983]\n",
      " [0.48417303 0.79401445 0.5764889  0.34206943]]\n",
      "Iteration 0\n",
      "\n",
      "Predicted Output: \n",
      "[[0.76883729 0.68987526]\n",
      " [0.80550308 0.71962289]\n",
      " [0.8006851  0.70888651]\n",
      " [0.8285938  0.73426905]]\n",
      "Actual output: \n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: \n",
      "0.31780017901073604\n",
      "\n",
      "\n",
      "Iteration 250\n",
      "\n",
      "Predicted Output: \n",
      "[[0.03849973 0.96132707]\n",
      " [0.89207693 0.10690209]\n",
      " [0.89202988 0.10691238]\n",
      " [0.13569405 0.86573032]]\n",
      "Actual output: \n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: \n",
      "0.010697787438138196\n",
      "\n",
      "\n",
      "Iteration 500\n",
      "\n",
      "Predicted Output: \n",
      "[[0.01600774 0.9837091 ]\n",
      " [0.94502741 0.05471682]\n",
      " [0.94502708 0.05471754]\n",
      " [0.06876965 0.93160418]]\n",
      "Actual output: \n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: \n",
      "0.0027451053784770165\n",
      "\n",
      "\n",
      "Iteration 750\n",
      "\n",
      "Predicted Output: \n",
      "[[0.01109431 0.98867059]\n",
      " [0.95888612 0.04097901]\n",
      " [0.95888687 0.04097854]\n",
      " [0.05138734 0.94881835]]\n",
      "Actual output: \n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: \n",
      "0.0015313524544941079\n",
      "\n",
      "\n",
      "Iteration 1000\n",
      "\n",
      "Predicted Output: \n",
      "[[0.00879825 0.99099989]\n",
      " [0.96586074 0.03404996]\n",
      " [0.96586154 0.03404933]\n",
      " [0.04265466 0.95748609]]\n",
      "Actual output: \n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Loss: \n",
      "0.0010543680483613021\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input data with targets\n",
    "X=np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float)\n",
    "y=np.array(([0, 1],[1, 0],[1, 0],[0, 1]), dtype=float)  \n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(t):\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "# Derivative of activation required for backpropagation\n",
    "def sigmoid_derivative(p):\n",
    "    return p * (1 - p)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x,y):\n",
    "        self.input = x\n",
    "        self.weights1= np.random.rand(self.input.shape[1],4) # 3x4: first layer with 3 units (3 features, 4 examples)\n",
    "        self.weights2 = np.random.rand(4,2) # 4x2 output layer: Num of examples in batch with a class prediction per example [0, 1] or [1, 0]. Make it 4x1 to change to regression.\n",
    "        self.y = y\n",
    "        self.output = np. zeros(y.shape)\n",
    "        \n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1)) # Matrix multiplication (dot acts a mat mult with matrices) 4x3 * 3x4 -> 4x4 matrix\n",
    "        self.layer2 = sigmoid(np.dot(self.layer1, self.weights2)) # 4x4 * 2x1 -> 4x1\n",
    "        return self.layer2\n",
    "        \n",
    "    def backprop(self):\n",
    "        # Chain rule on each layer?\n",
    "        d_weights2 = np.dot(self.layer1.T, 2*(self.y -self.output)*sigmoid_derivative(self.output))\n",
    "        d_weights1 = np.dot(self.input.T, np.dot(2*(self.y -self.output)*sigmoid_derivative(self.output), self.weights2.T)*sigmoid_derivative(self.layer1))\n",
    "    \n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.output = self.feedforward()\n",
    "        self.backprop()\n",
    "        \n",
    "\n",
    "# print(X.shape)\n",
    "# print(np.random.rand(X.shape[1],4))\n",
    "NN = NeuralNetwork(X,y)\n",
    "\n",
    "print(NN.weights1)\n",
    "\n",
    "layer1 = sigmoid(np.dot(X, NN.weights1))\n",
    "# print(sigmoid(np.dot(layer1, NN.weights2)))\n",
    "\n",
    "for i in range(1001): # 200 epochs\n",
    "    if i % 250 == 0: \n",
    "        print (\"Iteration \" + str(i) + \"\\n\")\n",
    "        print (\"Predicted Output: \\n\" + str(NN.feedforward()) + \"\\nActual output: \\n\" + str(y))\n",
    "        print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.feedforward())))) # mean sum squared loss\n",
    "        print (\"\\n\")\n",
    "  \n",
    "    NN.train(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
